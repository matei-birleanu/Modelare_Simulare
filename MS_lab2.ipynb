{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06cbbd5a-1354-4cb4-b3d0-b682677261f8",
      "metadata": {
        "id": "06cbbd5a-1354-4cb4-b3d0-b682677261f8"
      },
      "source": [
        "# Modeling and Simulation 3 - Data Compression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df1885a8-f6c3-4fb4-9b19-f40557411e16",
      "metadata": {
        "tags": [],
        "id": "df1885a8-f6c3-4fb4-9b19-f40557411e16"
      },
      "source": [
        "In computer science, data encoding refers to the process of converting data from one format to another. This is often done to optimize data storage, transmission, or processing. Data encoding can involve various techniques such as compression, encryption, and conversion of data between different character sets.\n",
        "\n",
        "Data encryption is a type of data encoding that is used to secure data by converting it into a form that can only be deciphered by authorized parties. This is often done to protect sensitive information such as passwords, credit card numbers, or confidential business data.\n",
        "\n",
        "Data encoding can also involve converting data between different character sets. For example, ASCII encoding is a way of representing text using a standard set of 128 characters, while Unicode encoding allows for the representation of characters from many different languages and scripts.\n",
        "\n",
        "Data compression is a form of data encoding that reduces the size of data without losing any important information. This is typically done to optimize storage capacity, reduce transmission time over a network, or improve processing efficiency.\n",
        "\n",
        "\n",
        "Data compression has many applications in computer science. For example, it can be used to reduce the size of files for storage on a hard drive or other storage media. It can also be used to reduce the amount of data that needs to be transmitted over a network, which can be particularly important in situations where bandwidth is limited.\n",
        "\n",
        "There are two main types of data compression:\n",
        "- Lossless compression and lossy compression. Lossless compression is a technique that reduces the size of data without any loss of information. This is typically done by removing redundant data or patterns in the data. Examples of lossless compression algorithms include ZIP, GZIP, and BZIP2.\n",
        "\n",
        "- Lossy compression, on the other hand, is a technique that reduces the size of data by discarding some information that is deemed to be less important. This can result in a smaller file size, but it may also result in a reduction in the quality of the data. Examples of lossy compression algorithms include JPEG for images and MP3 for audio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9d7dcc1-d9a7-448f-936e-1ada01f8ef07",
      "metadata": {
        "id": "e9d7dcc1-d9a7-448f-936e-1ada01f8ef07"
      },
      "source": [
        "## Huffman Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcc2d85-f964-4bb7-8c54-ee393f22c54a",
      "metadata": {
        "id": "4fcc2d85-f964-4bb7-8c54-ee393f22c54a"
      },
      "source": [
        "Lossless compression is a technique in computer science that reduces the size of data without any loss of information. There are several methods for achieving lossless compression, including run-length encoding, arithmetic coding, and Huffman coding.\n",
        "\n",
        "Huffman coding is a popular technique for lossless data compression that is widely used in many applications such as image and audio compression. It is a variable-length code that assigns shorter bit sequences to more frequently occurring characters and longer bit sequences to less frequently occurring characters.\n",
        "\n",
        "The Huffman encoding process involves several steps:\n",
        "\n",
        "1. Frequency Analysis: The frequency of occurrence of each character in the input data is analyzed.\n",
        "\n",
        "2. Build Huffman Tree: A binary tree is constructed based on the frequency of occurrence of each character. The tree is built using a greedy algorithm that minimizes the total encoding length of the data.\n",
        "\n",
        "3. Assign Codes: Each leaf node of the Huffman tree is assigned a unique bit pattern that represents the character it corresponds to. The bit pattern for each character is obtained by traversing the tree from the root to the leaf node.\n",
        "\n",
        "4. Encode Data: The input data is encoded using the bit patterns assigned to each character in the Huffman tree. This produces a compressed version of the data that can be stored or transmitted.\n",
        "\n",
        "The algorithm begins with building a binary tree, known as a Huffman tree, that represents the frequency of each symbol in the message. The tree is built recursively by merging the two least frequent symbols into a single node until all the symbols are combined into a single root node. The left child of each internal node represents the symbol with the higher frequency, while the right child represents the symbol with the lower frequency.\n",
        "\n",
        "Once the Huffman tree is constructed, a code is assigned to each symbol based on its position in the tree. Starting from the root node, traversing left or right represents a binary 0 or 1, respectively. The code assigned to each symbol is the path from the root node to that symbol in the tree. As a result, symbols with higher frequency are assigned shorter codes, while symbols with lower frequency are assigned longer codes.\n",
        "\n",
        "The final code is generated by replacing each symbol in the original message with its assigned code. The encoded message is a concatenation of these codes. To decode the message, the Huffman tree is reconstructed using the same frequency information used to build it, and the encoded message is traversed through the tree to find the corresponding symbols.\n",
        "\n",
        "The main advantage of Huffman coding is its ability to produce efficient compression with a low computational overhead. Because the encoding is based on the frequency of occurrence of characters in the data, it is particularly effective for compressing text-based data such as documents and emails.\n",
        "\n",
        "However, the compression ratio achieved by Huffman coding depends heavily on the frequency distribution of characters in the input data. If the input data has a uniform distribution of characters, Huffman coding may not be very effective. In such cases, other compression algorithms may be more suitable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560c830d-d8a7-4587-ad6e-7ecf7e6b1870",
      "metadata": {
        "id": "560c830d-d8a7-4587-ad6e-7ecf7e6b1870"
      },
      "source": [
        "![image.png](attachment:366d0a06-7628-4377-8151-d1367bd2e63f.png)\n",
        "\n",
        "The tree resulting from encoding \"this is an example of a huffman tree\" using Huffman codes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d37bc70-bf1e-4662-a4c7-203237ff5421",
      "metadata": {
        "id": "1d37bc70-bf1e-4662-a4c7-203237ff5421"
      },
      "source": [
        "### Task 1: Encode and Decode using Huffman codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bcf51c5b-9fc9-4564-a9c8-e0884d0f82e6",
      "metadata": {
        "tags": [],
        "id": "bcf51c5b-9fc9-4564-a9c8-e0884d0f82e6"
      },
      "outputs": [],
      "source": [
        "class HuffmanNode:\n",
        "    def __init__(self, freq, symbol=None):\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.freq = freq\n",
        "        self.symbol = symbol\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return self.left is None and self.right is None\n",
        "\n",
        "def build_huffman_tree(freq_map):\n",
        "    nodes = [HuffmanNode(freq, symbol) for symbol, freq in freq_map.items()]\n",
        "\n",
        "    while len(nodes) > 1:\n",
        "        nodes.sort(key=lambda x: x.freq)\n",
        "        left = nodes.pop(0)\n",
        "        right = nodes.pop(0)\n",
        "        parent = HuffmanNode(left.freq + right.freq)\n",
        "        parent.left = left\n",
        "        parent.right = right\n",
        "        nodes.append(parent)\n",
        "\n",
        "    return nodes[0]\n",
        "\n",
        "def traverse_huffman_tree(root, code_map, current_code=''):\n",
        "    if root is None:\n",
        "        return\n",
        "\n",
        "    if root.is_leaf():\n",
        "        code_map[root.symbol] = current_code\n",
        "    else:\n",
        "        traverse_huffman_tree(root.left, code_map, current_code + '0')\n",
        "        traverse_huffman_tree(root.right, code_map, current_code + '1')\n",
        "\n",
        "def huffman_encode(text):\n",
        "    freq_map = {}\n",
        "    for char in text:\n",
        "        freq_map[char] = freq_map.get(char, 0) + 1\n",
        "\n",
        "    root = build_huffman_tree(freq_map)\n",
        "    code_map = {}\n",
        "    traverse_huffman_tree(root, code_map)\n",
        "\n",
        "    encoded = ''.join(code_map[char] for char in text)\n",
        "    return encoded, code_map\n",
        "\n",
        "def huffman_decode(encoded, code_map):\n",
        "    reverse_code_map = {code: symbol for symbol, code in code_map.items()}\n",
        "    current_code = ''\n",
        "    decoded = []\n",
        "\n",
        "    for bit in encoded:\n",
        "        current_code += bit\n",
        "        if current_code in reverse_code_map:\n",
        "            decoded.append(reverse_code_map[current_code])\n",
        "            current_code = ''\n",
        "\n",
        "    return ''.join(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d6e6bb66-ec18-4e67-a0a8-918e70051a9a",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6e6bb66-ec18-4e67-a0a8-918e70051a9a",
        "outputId": "62868816-ef6d-4212-9323-2366d73ccb35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11101111101011000000111001010011\n",
            "hello world\n"
          ]
        }
      ],
      "source": [
        "data = 'hello world'\n",
        "\n",
        "encoded_data_huff, encoding_dict = huffman_encode(data)\n",
        "print(encoded_data_huff)\n",
        "decoded_data_huff = huffman_decode(encoded_data_huff, encoding_dict)\n",
        "print(decoded_data_huff)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45464a5a-7340-4d42-996e-1deab6ef7b96",
      "metadata": {
        "id": "45464a5a-7340-4d42-996e-1deab6ef7b96"
      },
      "source": [
        "The correct encoding should be: 11101111101011000000111001010011"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa7e70b-7480-46b4-9516-4004430783b4",
      "metadata": {
        "id": "eaa7e70b-7480-46b4-9516-4004430783b4"
      },
      "source": [
        "While Huffman coding is a popular and efficient technique for lossless data compression, it has several drawbacks and limitations that should be considered when choosing a compression algorithm.\n",
        "\n",
        "1. Encoding and decoding complexity: The Huffman encoding process requires constructing a frequency tree, assigning codes to each character, and encoding the input data using those codes. Similarly, the decoding process requires traversing the Huffman tree to reconstruct the original data. While these steps can be performed efficiently, they still add some computational overhead that can be a concern in certain applications.\n",
        "\n",
        "2. Compression ratio limitations: Huffman coding works best when the input data has a non-uniform frequency distribution of characters, allowing for more efficient encoding of the most frequently occurring characters. However, when the input data has a relatively uniform distribution, Huffman coding may not achieve significant compression. In addition, Huffman coding may not be optimal for encoding data with complex structures, such as images or video.\n",
        "\n",
        "3. Lack of error tolerance: Huffman coding is a lossless compression technique, which means that the original data can be perfectly reconstructed from the compressed data. However, this also means that any errors introduced during transmission or storage can completely corrupt the compressed data and render it unusable. In contrast, some lossy compression techniques, such as JPEG or MP3, are more tolerant to errors and can still produce acceptable output even when some data is lost or corrupted.\n",
        "\n",
        "4. Need for adaptive encoding: The Huffman encoding process requires knowing the frequency distribution of characters in the input data beforehand, which may not always be possible or practical. In some cases, an adaptive encoding approach may be more appropriate, where the encoding algorithm updates its character frequency estimates dynamically as it encounters new data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cef1dc24-f129-4d99-80a1-8752202550f1",
      "metadata": {
        "id": "cef1dc24-f129-4d99-80a1-8752202550f1"
      },
      "source": [
        "## N-gram Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92823b7f-56dd-4b82-9f64-d09b2ceb299c",
      "metadata": {
        "id": "92823b7f-56dd-4b82-9f64-d09b2ceb299c"
      },
      "source": [
        "Lossy data compression techniques are methods used to reduce the size of data by discarding some information that is deemed unnecessary for the intended use of the data. These techniques work by identifying and removing redundancies or patterns in the data, which can result in a significant reduction in the size of the compressed data.\n",
        "\n",
        "In natural language processing, an n-gram is a contiguous sequence of n items (words, letters, or phonemes) within a text. For example, given the sentence \"The quick brown fox jumps over the lazy dog\", some 2-grams (also called bigrams) would be \"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", \"the lazy\", and \"lazy dog\". Similarly, some 3-grams (trigrams) would be \"The quick brown\", \"quick brown fox\", \"brown fox jumps\", \"fox jumps over\", \"jumps over the\", \"over the lazy\", and \"the lazy dog\". N-grams are widely used in natural language processing for language modeling, text classification, and machine translation, among other tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b27580-7f9d-45aa-9ffa-472c36e463f3",
      "metadata": {
        "id": "c8b27580-7f9d-45aa-9ffa-472c36e463f3"
      },
      "source": [
        "![image.png](attachment:d7ce57a7-07f4-470b-b3f5-b4fd63a4d441.png)\n",
        "\n",
        "[N-gram dictionary compression](https://www.hindawi.com/journals/cin/2016/9483646/) is a specific type of lossy data compression technique that works by creating a dictionary of frequently occurring sequences of characters or words, called n-grams. This dictionary is used to replace the original sequences in the data with a shorter index, resulting in a reduction in the overall size of the data. However, since this technique discards some information, it is considered a lossy compression technique.\n",
        "\n",
        "The degree of information loss in lossy compression techniques can vary depending on the specific technique used, the compression level selected, and the nature of the data being compressed. In some cases, the loss of information may not be noticeable or may not significantly affect the intended use of the data. In other cases, the loss of information may be significant and may affect the quality or integrity of the data.\n",
        "\n",
        "N-gram dictionary compression is a lossy data compression technique that works by creating a dictionary of frequently occurring sequences of characters or words, called n-grams. The n-gram dictionary is created by analyzing the input data and identifying frequently occurring sequences of n consecutive characters or words. The n-grams are then assigned a unique index or code and stored in the dictionary.\n",
        "\n",
        "To compress the data, each n-gram in the input data is replaced with its corresponding index from the dictionary. This process effectively reduces the size of the data by replacing long sequences of characters or words with shorter codes. The compressed data is then stored or transmitted using the compressed representation, which requires less storage space or transmission bandwidth."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d806e2-a330-4b92-9be9-550c8e427cab",
      "metadata": {
        "id": "f9d806e2-a330-4b92-9be9-550c8e427cab"
      },
      "source": [
        "![image.png](attachment:cb2610bd-72cf-45fb-bf07-e99a63fe1d57.png)\n",
        "\n",
        "The degree of compression achieved using n-gram dictionary compression depends on several factors, including the length of the n-grams used, the frequency of the n-grams in the input data, and the size of the dictionary. In general, longer n-grams can capture more context and result in higher compression ratios, but they are less likely to occur frequently in the input data. On the other hand, shorter n-grams may result in lower compression ratios, but they are more likely to occur frequently in the input data.\n",
        "\n",
        "One of the advantages of n-gram dictionary compression is its simplicity and efficiency. The compression and decompression processes are relatively fast and require minimal computational resources. This makes it suitable for use in applications with limited resources, such as mobile devices or embedded systems.\n",
        "\n",
        "However, one of the main drawbacks of n-gram dictionary compression is the loss of information that occurs during compression. Since the technique works by replacing sequences of characters or words with shorter codes, some information is inevitably lost in the process. The degree of information loss depends on several factors, including the size of the dictionary, the length of the n-grams used, and the nature of the input data. Therefore, n-gram dictionary compression is most suitable for applications where some loss of information is acceptable, such as text or speech data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "225d618d-d899-47a8-80ae-024fc9df050b",
      "metadata": {
        "tags": [],
        "id": "225d618d-d899-47a8-80ae-024fc9df050b"
      },
      "outputs": [],
      "source": [
        "def encode_ngram(text, n):\n",
        "    model = {}\n",
        "    encoded_text = []\n",
        "    for i in range(len(text) - n + 1):\n",
        "        ngram = text[i:i+n]  # Extract the n-gram at the current position\n",
        "        if ngram not in model:\n",
        "            model[ngram] = len(model)  # Assign an integer value to the n-gram and store it in the model\n",
        "        encoded_text.append(model[ngram])  # Append the integer representation of the current n-gram to the encoded text\n",
        "    return encoded_text, model\n",
        "\n",
        "def decode_ngram(n, encoded_text, model):\n",
        "    decoded_text = \"\"\n",
        "    inverse_model = {v: k for k, v in model.items()}  # Create an inverse model to map integer representations back to n-grams\n",
        "    for i in range(len(encoded_text)):\n",
        "        ngram = inverse_model[encoded_text[i]]  # Look up the corresponding n-gram for the current integer\n",
        "        decoded_text += ngram  # Reconstruct the part of the text using the ngram\n",
        "    return decoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6b5eca13-dc82-4d94-bac7-b9621985c973",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b5eca13-dc82-4d94-bac7-b9621985c973",
        "outputId": "50f3774c-392b-43cc-82f3-86fb18443985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "heelllloo  wwoorrlld\n"
          ]
        }
      ],
      "source": [
        "# Define the input data and frequency table\n",
        "n = 2\n",
        "\n",
        "encoded_data_ngram, model = encode_ngram(data, n)\n",
        "print(encoded_data_ngram)\n",
        "\n",
        "decoded_data_ngram = decode_ngram(n, encoded_data_ngram, model)\n",
        "print(decoded_data_ngram)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ece8966-a8aa-4b62-b88a-c9b7530cdc4d",
      "metadata": {
        "id": "1ece8966-a8aa-4b62-b88a-c9b7530cdc4d"
      },
      "source": [
        "There are several disadvantages of n-gram dictionary encoding:\n",
        "\n",
        "- Memory requirements: n-gram dictionary encoding requires storing the entire dictionary, which can be memory-intensive for large datasets. As n increases, the number of possible n-grams grows exponentially, which can lead to memory limitations.\n",
        "\n",
        "- Lossy compression: n-gram dictionary encoding is a lossy compression technique, meaning that some information is lost in the compression process. This can result in a loss of accuracy in certain applications, particularly for tasks that require precise text representations.\n",
        "\n",
        "- Limited context: n-gram models only consider the context of the current n-gram, which may not capture longer-range dependencies or contextual information in the text. This can lead to inaccurate or incomplete representations of the input text.\n",
        "\n",
        "- Generalization: n-gram models can struggle to generalize to unseen data or novel contexts, particularly if the training data is limited or biased. This can limit the utility of n-gram models in certain applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a377df46-db45-4f35-ae41-4e33dc2a9b07",
      "metadata": {
        "id": "a377df46-db45-4f35-ae41-4e33dc2a9b07"
      },
      "source": [
        "Now, let's compare the compression performance of the two methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b856b2f6-c49f-48e5-91c4-5f15326ca9bb",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b856b2f6-c49f-48e5-91c4-5f15326ca9bb",
        "outputId": "3486551f-a004-4204-af4a-235313390438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compression ratio Huffman: 2.75\n",
            "Compression ratio n-gram: 2.20\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def compression_ratio(data, compressed_data):\n",
        "    original_bits = len(data) * 8  # Assuming 8 bits per character for the original data\n",
        "    compressed_bits = len(compressed_data) * math.ceil(math.log2(len(set(compressed_data))))  # Bits required for the compressed data\n",
        "\n",
        "    return original_bits / compressed_bits\n",
        "\n",
        "# Example usage\n",
        "ratio = compression_ratio(data, encoded_data_huff)\n",
        "print(f\"Compression ratio Huffman: {ratio:.2f}\")\n",
        "\n",
        "# Compression ratio for n-gram dictionary compression\n",
        "ratio = compression_ratio(data, encoded_data_ngram)\n",
        "print(f\"Compression ratio n-gram: {ratio:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}